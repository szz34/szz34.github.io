---
title: 'wandb坑：为什么每次调参都用的默认参数配置？'
date: 2024-02-02
permalink: /posts/2024/02/wandb/
tags:
  - pytorch
  - wandb
---

[wandb](https://wandb.ai/site)是一款超好用的调参工具，当你设定好参数候选集后，程序会自动帮你每次选择一组参数来运行代码，最后的结果还可以生成非常清晰的可视化，具体如何使用请大家自行查阅其官方网站和相关资料，这里就不赘述。

这里主要想分享我在使用wandb调参时遇到的一个大坑，同时也在网上看到很多学习wandb的小伙伴都遇到的一个问题，就是：为什么我明明设置好了每个参数的候选值，但wandb每次运行选择的参数都是一样的？

举个例子：我想要调的参数有learning_rate（我设置候选值为：1e-3, 1e-4, 1e-5）和batch_size（设置候选值为：16, 32, 64）。我希望的是第一次运行，程序能够选择(1e-3, 16)这组参数运行看看效果，第二次程序就换一组参数(1e-4, 64)进去运行看看效果。但是通过实际打印的参数值会发现，程序每次都在用(1e-3, 16)这组你的默认参数在运行程序，并没有每次随机选择参数。

（一开始我没有发现这个问题，因为我的程序没有设置随机种子，所以每次跑出来结果不一样，我误认为程序每次选择了不同的参数组。直到我设置好了种子，发现每次结果都一样，才发现了这个问题）。

导致这个问题的原因其实很简单，我相信很多小伙伴在一开始学习使用wandb时，搜到的教程都是吃货君发布的 [30分钟吃掉wandb可视化自动调参](https://github.com/lyhue1991/eat_pytorch_in_20_days/blob/master/A-7%2C30%E5%88%86%E9%92%9F%E5%90%83%E6%8E%89wandb%E5%8F%AF%E8%A7%86%E5%8C%96%E8%87%AA%E5%8A%A8%E8%B0%83%E5%8F%82.ipynb) 这个教程。但实际上该教程给出的示例代码中存在问题：

如下所示，代码在config中配置了每个要调的参数的默认值。

```
from argparse import Namespace

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

#初始化参数配置
config = Namespace(
    project_name = 'wandb_demo',
    
    batch_size = 512,
    
    hidden_layer_width = 64,
    dropout_p = 0.1,
    
    lr = 1e-4,
    optim_type = 'Adam',
    
    epochs = 15,
    ckpt_path = 'checkpoint.pt'
)
```

然后在sweep_config['parameters']中设置了每个参数的候选集。

```
sweep_config['parameters'] = {}

# 固定不变的超参
sweep_config['parameters'].update({
    'project_name':{'value':'wandb_demo'},
    'epochs': {'value': 10},
    'ckpt_path': {'value':'checkpoint.pt'}})

# 离散型分布超参
sweep_config['parameters'].update({
    'optim_type': {
        'values': ['Adam', 'SGD','AdamW']
        },
    'hidden_layer_width': {
        'values': [16,32,48,64,80,96,112,128]
        }
    })

# 连续型分布超参
sweep_config['parameters'].update({
    
    'lr': {
        'distribution': 'log_uniform_values',
        'min': 1e-6,
        'max': 0.1
      },
    
    'batch_size': {
        'distribution': 'q_uniform',
        'q': 8,
        'min': 32,
        'max': 256,
      },
    
    'dropout_p': {
        'distribution': 'uniform',
        'min': 0,
        'max': 0.6,
      }
})
```

然而，在控制wandb整个代码运行的train函数中，传入的参数配置为config，也就是默认的参数配置。

```
def train(config = config):
    dl_train, dl_val = create_dataloaders(config)
    model = create_net(config); 
    optimizer = torch.optim.__dict__[config.optim_type](params=model.parameters(), lr=config.lr)
    #======================================================================
    nowtime = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    wandb.init(project=config.project_name, config = config.__dict__, name = nowtime, save_code=True)
    model.run_id = wandb.run.id
    #======================================================================
    model.best_metric = -1.0
    for epoch in range(1,config.epochs+1):
        model = train_epoch(model,dl_train,optimizer)
        val_acc = eval_epoch(model,dl_val)
        if val_acc>model.best_metric:
            model.best_metric = val_acc
            torch.save(model.state_dict(),config.ckpt_path)   
        nowtime = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        print(f"epoch【{epoch}】@{nowtime} --> val_acc= {100 * val_acc:.2f}%")
        #======================================================================
        wandb.log({'epoch':epoch, 'val_acc': val_acc, 'best_val_acc':model.best_metric})
        #======================================================================        
    #======================================================================
    wandb.finish()
    #======================================================================
    return model   

#model = train(config)
```

所以程序每次实际上都是按照config的配置参数来运行的，也就是每次都用的你默认的参数值。

如何解决该问题？可以参照另外一个教程的写法： [基于wandb sweeps的pytorch超参数调优实验](https://zhuanlan.zhihu.com/p/436385177) 。即：不设置默认的参数，在设置好参数的候选集后，在train函数中传入候选集的配置：

```
def train(config=None):
    with wandb.init(config=None):
        config = wandb.config
```

最后，大家也最好在程序里print一下相关的参数值，看看每次程序运行是不是选择了不同的参数组合。

------
